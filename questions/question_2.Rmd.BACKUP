# 2. Randomization Inference Practice

Suppose that you've been hired as the data scientist at a quack nootropics company. Despite their fraudulent intent, you're dedicated to doing good data science. Or, at least science as good as you can. 

Their newest serum, *kniht* purports to raise its users' executive function. You think that it is just amphetamines. 

As the data scientist for the company, you convince them to conduct a trial. Great! The good news is this:

- Each person is measured twice.
- Before one of the measurements, they are given a placebo. Before the other of the measurements they are given *kniht*. 
- You ask for instrumentation on two concepts: 
  - Creativity, measured as number of proposed alternative uses of an object. (This is a classic, test of "creativity", proposed by J.P. Guilford. For example, how many things can you propose doing with a camera tripod? )
  - Physical Arousal, measured through skin conductance (i.e. how sweaty is someone). 
  
The bad news is this: The company knows that they're selling nonsense, and they don't want you to be able to prove it. They reason that if they provide you only six test subjects, that you won't be able to prove anything, and that they can hide behind a "fail-to-reject" claim. 

```{r}
kniht <- data.table(
  person  = rep(LETTERS[1:6], each = 4), 
  treat   = rep(0:1, each = 2), 
  measure = rep(c('creative', 'sweat'))
)


kniht[measure == 'creative' & treat == 0, 
      value := c(10, 13, 14, 16, 25, 40)]
kniht[measure == 'creative' & treat == 1, 
      value := c(12, 11, 13, 20, 21, 46)]
kniht[measure == 'sweat' & treat == 0, 
      value := c(0.4, 0.7, 0.3, 0.8, 1.0, 1.4)]
kniht[measure == 'sweat' & treat == 1, 
      value := c(0.4, 0.7, 2.0, 0.9, 1.6, 2.2)]
```

Conduct the following tests. 

1. Conduct the appropriate t-test that respects the repeated-measures nature of the data (is this a paired or independent samples t-test?) for both the `creative` and the `sweat` outcomes. After you conduct your tests, write a narrative statement about what you conclude. 

```{r creative t-test}

library(dplyr)

creative_tr <- kniht %>% filter(measure == 'creative' & treat == 1)
creative_untr <- kniht %>% filter(measure == 'creative' & treat == 0)

t_test_creative <- t.test(creative_tr$value, creative_untr$value, paired = TRUE, alternative = "greater")
t_test_creative
```


```{r sweat t-test}

sweat_tr <- kniht %>% filter(measure == 'sweat' & treat == 1)
sweat_untr <- kniht %>% filter(measure == 'sweat' & treat == 0)

t_test_sweat <- t.test(sweat_tr$value, sweat_untr$value, paired = TRUE, alternative = "greater")
t_test_sweat
```

We want to run a paired samples t-test, since our samples include both placebo and an actual treatment. It appears as though the p-value of the sweatiness is `r t_test_sweat.p.value`, reasonably low for such a small number of participants. p-value of the creativity though is `r t_test_creative.p.value`, which is quite high.

2. Conduct the appropriate randomization inference test that respects the repeated-measures nature of the data. After you conduct your tests, write a narrative statement about what you conclude.  

```{r creative ri}
# do your work, and then save your computed p-value in the object

calcuate_measure_ate <- function(measure_var) {
  people = unique(kniht$person)
  t_effect = 0
  for( p in people ) {
    tmp <- kniht %>% filter(person == p & measure == measure_var)
    t_effect <- t_effect + (tmp[treat == 1]$value - tmp[treat == 0]$value)
  }
  return(t_effect/length(people))
}

random1 <- function() {
  sample(c(rep(1, 3), rep(0, 3)))
}

creative_value_for_person <- function(p)
{
  tmp <- kniht %>% filter(person == p & measure == "creative")
  return(tmp[treat == 1]$value - tmp[treat == 0]$value)
}

creative_ate     <- calcuate_measure_ate("creative")
creative_ri      <- function() {
  
  po_treated <- data.table(person = unique(kniht$person), treatment = random1())
  po_treated_values <- lapply(unique(kniht$person), creative_value_for_person)
  po_treated$value <- unlist(po_treated_values) # ah, the principle of most surprise

  t_effect = mean(po_treated[treatment == 1]$value) - mean(po_treated[treatment == 0]$value) 
  return(t_effect)
  
}

creative_ri_distribution <- replicate(1000, creative_ri())

hist(creative_ri_distribution, main = "Histogram of Creative Results", breaks = 100)
abline(v = creative_ate, col='red')

creative_p_value <- mean(creative_ate < creative_ri_distribution)
```

```{r sweat ri}
# do your work, and then save your computed p-value in the object

sweat_value_for_person <- function(p)
{
  tmp <- kniht %>% filter(person == p & measure == "sweat")
  return(tmp[treat == 1]$value - tmp[treat == 0]$value)
}

sweat_ate     <- calcuate_measure_ate('sweat')
sweat_ri      <- function() {
  
  po_treated <- data.table(person = unique(kniht$person), treatment = random1())
  po_treated_values <- lapply(unique(po_treated$person), sweat_value_for_person)
  po_treated$value <- unlist(po_treated_values) # ah, the principle of most surprise

  t_effect = mean(po_treated[treatment == 1]$value) - mean(po_treated[treatment == 0]$value) 
  return(t_effect)
  
}

sweat_ri_distribution <- replicate(1000, sweat_ri())

hist(sweat_ri_distribution, main = "Histogram of Creative Results", breaks = 100)
abline(v = sweat_ate, col="red")

# this seems wrong but I can't figure out why
sweat_p_value <-  mean(which(sweat_ate < sweat_ri_distribution))
```

**Esteemed Colleagues of Kniht,**

**I have examined our results using the study data. If we have a hypothesis that  Kniht definitely raises users creativity and a null hypothesis that says that Kniht is completely ineffective at raising users creativity we should be able to test this by running a random inference test that looks at our data from both perspectives. This random inference tests how likely it is that our two results are the result of Kniht being a powerful stimulant to creativity. Unfortunately the p-value of our creativity test, that is, the likelihood that our results could be completely accidental, appears to be `r creative_p_value`. Slightly more convincing is the likelihood that Kniht does in fact cause some increase in sweatiness, as the p-value of that test is `r sweat_p_value`. We may want to run further and more detailed studies using more research subjects. Or we should cash out. In which case I would like a raise.**

3. Which of these tests are more appropriate to the task at hand, and why? Based on the tests that you have run, what do you conclude about the effectiveness of *kniht*? 

**I think a one-tailed test is more appropriate in both circumstances as the claim is that Kniht increases the creativity and the evidence seems to suggest that it increases the sweatiness.**
